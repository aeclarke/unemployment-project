---
title: 'Determinants of Unemployment Rate: Final Project Report'
author: 'Ashley Clarke'
date: 'Link to Code Repository [on Github](https://github.com/aeclarke/unemployment-project).'
output:
  bookdown::pdf_document2:
    number_sections: yes
    toc: yes
    toc_depth: '2'
  html_document:
    highlight: haddock
    number_sections: yes
    theme: lumen
    toc: no
    toc_depth: 4
    toc_float: yes
urlcolor: blue
---

```{r setup, include=FALSE}
options(scipen = 0, digits = 3)  # controls number of significant digits printed
library(dplyr)
library(tidyverse)
library(ggcorrplot) 
library(kableExtra)
```


```{r download-var-names, include=FALSE}
var_names = read_tsv("~/Desktop/STAT471/unemployment-project/data/clean/var_names.tsv")
```

\newpage

# Executive Summary {-}

## Problem {-} 

Unemployment rates fluctuate over time as the economy goes through periods of recession and depression. Every month the Bureau of Labor Statistics releases an "Unemployment Situation Summary" and a "Gross Domestic Product Summary". While it is often assumed that factors in these two reports are linked to the United States unemployment rate, analysts often do not know which of the thousands of variables in these reports they should focus their attention on. Variables examined range from net government lending to the number of employees by industry sector. While using data from 1960 to 2020 inherently ignores shifts in employment patterns across time, the purpose of this report is to determine which factors have historically been predictive of the U.S. unemployment rates, not to predict future unemployment rates. Therefore, this report aims to identify and explain how different factors in these two reports relate to the U.S. unemployment rate.

## Data {-}

>(1) Employment Situation Summary^[https://www.bls.gov/news.release/empsit.nr0.htm]

>(2) Gross Domestic Product^[https://www.bea.gov/data/gdp/gross-domestic-product]

>(3) Additional Federal Reserve of Economic Data Variables: Unemployment Rate, Inflation, Federal Funds Rate 

My dataset combines data from two sources. First, I pulled the "Unemployment Situation Summary" and the "Gross Domestic Product Summary" time-series data sets from the U.S. Bureau of Labor Statistics from 1954 to 2021. Next, I pulled three additional variables from the Federal Reserve Bank of St. Louis (unemployment rate, inflation, and federal funds rate). For the two time-series data sets, I pulled every key variable available. My primary response variable of interest was the unemployment rate, which is defined as the number of unemployed persons as a percentage of the labor force. While the original data set has 804 observations and 2004 variables, the clean data set has 732 observations and 160 variables.   

## Analysis {-}

Before exploring the dataset or running any analyses, I split the data into train and test data sets. I trained my model using the training data set and reserved the test dataset for measuring model performance. Next, I accessed correlations between variables and found multicollinearity issues, which I systematically dealt with. Next, I explored my data to look for relationships between variables and the response and checked to make sure all linear regression assumptions were met. To determine which features are predictive of the unemployment rate, I built six different cross-validated models: ordinary least squares, ridge regression, LASSO regression, elastic net regression, random forest, and boosting. Of the regression models, the ordinary least squares (OLS) regression has the lowest test error. The boosted model had the lowest test error of all models (and of the tree-based models). Finally, I drew conclusions based on what I learned from my models.  

## Conclusions {-}

I found that both regression and tree-based methods found similar variables to be strong predictors of the unemployment rate. Specifically, the boosted model found depreciation of fixed assets and the number of mining/logging employees to be the strongest predictors, revealing how changes in the number of employees for “blue-collar” professions are more predictive than changes in the number of employees for “white collar” professions. I also found that unemployment rises as government debt and spending rise. Additionally, I found that the unemployment rate falls when net exports as a percentage of GDP, money invested in fixed assets, and corporate profits increase. I hope this analysis can reframe how economists and analysts think about unemployment, both in the context of what signals potentially high future unemployment and the effects of unemployment. 

# Introduction {-}

## Background information {-}

People are classified as unemployed if they do not have a job, have actively looked for work in the prior four weeks, and are currently available for work^[ "How the Government Measures Unemployment". www.bls.gov.]. Only people classified as part of the civilian labor force are included in the unemployment rate metric. 

Unemployment rates fluctuate over time. During periods of recession and depression, unemployment is high. During periods of economic growth, unemployment tends to be lower. For example, the unemployment rate was 25% during the Great Depression, 10.8% in November 1982, and 14.7% in April 2020.^["Unemployment is nearing Great Depression levels. Here’s how the eras are similar — and different". https://www.cnbc.com/2020/05/19/unemployment-today-vs-the-great-depression-how-do-the-eras-compare.html]. Unemployment rates in the late 1990s and into the mid-2000s were low by historical standards. The unemployment rate was below 5% from 1997 to 2000 and near 5% during almost all of 2006–2007^["Patterns of Unemployment",https://opentextbc.ca/principlesofeconomics/chapter/21-2-patterns-of-unemployment/ ].

Unemployment generally falls during periods of economic prosperity and rises during recessions, creating significant pressure on public finances as tax revenue falls and social safety net costs increase. Government spending and taxation decisions (fiscal policy) and U.S. Federal Reserve interest rate adjustments (monetary policy) are crucial tools for managing the unemployment rate. There may be an economic trade-off between unemployment and inflation, as policies designed to reduce unemployment can create inflationary pressure, and vice versa. The U.S. Federal Reserve (the Fed) has a dual mandate to achieve full employment while maintaining a low rate of inflation as shown in (Figure  \@ref(fig:dual-mandate)). Historically, the Fed has targeted a 5% unemployment rate and 2% inflation. 

```{r dual-mandate, fig.align='center', fig.cap = "The Federal Reserve has a dual mandate to maintain low inflation and to achieve full employment", out.width="70%", echo = FALSE}
knitr::include_graphics("~/Desktop/STAT471/unemployment-project/results/dual-mandate.png")
```

There are a variety of measures used to track the state of the U.S. labor market. The Bureau of Labor Statistics provides a "chartbook" displaying the major employment-related variables in the economy ^["Current Population Survey (CPS)". Bls.gov.]. Members of the Federal Reserve also give speeches and Congressional testimony that explain their views of the economy, including the labor market.^["The Fed - Speeches of Federal Reserve Officials". Federalreserve.gov.].  

This report focuses on two reports released by the Bureau of Labor Statistics. For background, U.S. employment statistics are reported by government and private primary sources monthly, and these metrics are widely quoted in the news. These sources use a variety of sampling techniques that yield different measures. While the U.S. Bureau of Labor Statistics (BLS) provides a monthly "Employment Situation Summary", analysts often do not know which factors, amongst the hundreds provided, drive unemployment rates.  Additionally, employment trends can be analyzed by looking at the state of the economy. The BLS also provides a "Gross Domestic Product Summary" monthly, which provides the real gross domestic product (GDP) and other features related to GDP. 

## Analysis goals {-}

Given my knowledge that unemployment rates rise and fall in response to economic conditions, I sought to investigate which factors in the "Employment Situation Summary" and "Gross Domestic Product Summary" are predictive of unemployment. Additionally, I determined whether these factors were more or less predictive than inflation and the federal funds rate. The purpose of this analysis is not necessarily to predict future unemployment rates. Instead, my goal is to determine which features have been predictive of the U.S. unemployment rate in the past. To achieve this goal I will predict based on the 158 variables outlined in Table \@ref(tab:vars-appendix), these variables consist of employment-related variables and economic-related variables. I will consider my analysis a success if I can identify a set of variables that are found to be predictive of the U.S. unemployment rate using multiple modeling techniques. Additionally, I will determine which model best matches the underlying trend in the data by calculating test error. In addition to calculating test root-mean-square-error (RMSE) for each of my models, I will calculate the test RMSE error for an intercept-only model as a baseline.

## Significance {-}

This analysis goal is important to address in the context of application because determining which factors are linked to high unemployment can help shape fiscal and monetary policy. Additionally, I hope my analysis will allow analysts and policymakers to recognize signals of high unemployment, which can lead to significant economic downturns. Since many explanatory variables in this analysis relate to specific industry sectors, my analysis also aims to identify which sectors of the economy are most affected by changes to the unemployment rate. Lastly, I hope to improve the interpretability of the BLS's Employment Situation and GDP reports for all interested parties. 

# Data {-}

## Data sources {-}

**The raw dataset includes merged data from three sources from June 1954 to November 2021**

(1) Unemployment Situation Summary^[https://www.bls.gov/news.release/empsit.nr0.htm]

> Each month, the Bureau of Labor Statistics publishes the Employment Situation Summary report based on information from the prior month. The data for the report is derived primarily from two sources: a survey of approximately 60,000 households, or about 110,000 individuals (household survey), and an establishment survey of over 651,000 worksites.

(2) Gross Domestic Product Summary^[https://www.bea.gov/data/gdp/gross-domestic-product]

> Each month, the Bureau of Labor Statistics publishes the Gross Domestic Product Summary report based on information from the prior month. The report includes Gross Domestic Product (GDP), which is a comprehensive measure of U.S. economic activity. GDP measures the value of the final goods and services produced in the United States (without double counting the intermediate goods and services used up to produce them). The report includes variables relating to GDP. 

(3) Additional Federal Reserve of Economic Data Variables: Unemployment Rate^[https://fred.stlouisfed.org/series/UNRATE], Inflation^[https://fred.stlouisfed.org/series/FPCPITOTLZGUSA], Federal Funds Rate^[https://minds.wisconsin.edu/bitstream/handle/1793/77330/Federal%20Funds%20Rate.pdf?sequence=1&isAllowed=y] 

**Process of downloading data**

The data was collected using the fredr package^[https://cran.r-project.org/web/packages/fredr/vignettes/fredr.html], which provides a complete set of R bindings to the Federal Reserve of Economic Data (FRED) RESTful API, provided by the Federal Reserve Bank of St. Louis. The fredr package allowed me to search for and fetch time series observations as well as associated metadata within the FRED database. Since FRED organizes their data using variable ids, I downloaded time series observations from all variable ids in the Employment Situation and Gross Domestic Products reports, which represents over 2000 variables, from June 1954-November 2021. Additonally, I downloaded the U.S. unemployment rate (response variable), inflation rate, and federal funds rate. Before cleaning, the data set consisted of 804 observations and 2004 variables. 

Due to the size of the data set, the data set takes around 5-10 minutes to download from FRED. Also, since the fredr package has a limit of 120 requests / minute, it might take longer than expected for the data to load.  
 
## Data cleaning {-}

Three critical issues were resolved during the data cleaning phase:

> (1) Features have not been reported all years

> (2) Features are reported in different time increments: monthly, quarterly, and yearly 

> (3) Many of the features are highly correlated with each other since duplicate features are included

**1. Timeframe Issues: Not every feature has been reported since 1954**

While the unemployment rate has been reported monthly since June 1954, many other features have not been reported for the entire timeframe. Additionally, certain metrics have not yet been reported for 2021. Therefore, I decided to keep only observations from January 1960 to December 2020. Features that have not been reported since 1960 were dropped. 

**2. Reporting frequency: Not all features are reported monthly**

While both the Employment Situation and Gross Domestic Product reports are released monthly, not every feature is updated monthly. Many features are reported either quarterly or yearly. This issue was identified by examining the number of observations per feature. I noticed that many features had eith 61 or 244 complete observations as shown in (Figure  \@ref(fig:observations-per-feature)). 

This makes sense because the time frame of the dataset corresponds to 61 years and 244 quarters (3 months each). To impute the missing values for yearly data, I set every month in the yearly equal to the yearly metric. For missing quarterly data, I set the next two months equal to the quarterly metric. While I recognize that this is not a perfect way to impute these features, I believe it is better than dropping entire columns or rows. After imputing missing values for quarterly and yearly data, I dropped all columns with NA values, which left me with 831 features. 

```{r observations-per-feature, fig.align='center', fig.cap = "Histogram of the number of observations per feature that are complete. There are peaks at 61 (number of years) and 244 (number of quarters)", out.width="50%", echo = FALSE}
knitr::include_graphics("~/Desktop/STAT471/unemployment-project/results/histogram_na_count.png")
```

**3. Duplicate features: Some features represent the same metric with minor adjustments**

In their monthly reports, FRED makes minor adjustments to metrics and reports them as separate features. For instance, both seasonally adjusted and non seasonally adjusted numbers are reported for most metrics. To eliminate the double-counting variables, if a variable and another variable have a higher than 0.9 correlation with each other, I only kept one of the variables. Correlation is more informative than covariance when deciding which variables to remove because it does not depend on the scales of the features and always lies in the interval [-1,1]. The closer the correlation is to the endpoints of this interval, the more strongly the features are related. If the correlation is very high, it is likely the variables measure the same thing. Additionally, due to potential multicolinearity issues, I decided that it was better to remove features that are highly correlated with each other. After removing variables that are highly correlated, there are 160 remaining features. 

**Removed columns were the standard deviation is equal to zero**

I calculated the standard deviations of all variables. If a variable had a standard deviation of 0, I removed it because it is a meaningless feature. 

**Next, I examined the independence of observations**

Since each observation is indexed by month, I was concerned that samples of consecutive months would be strongly correlated (i.e. the unemployment rate of a given month would depend largely on the unemployment rate from the previous month). Intuitively, a time series is weakly dependent if events in the past have only a small influence on the value of the time series at the present moment^[https://towardsdatascience.com/time-series-analysis-part-i-3be41995d9ad]. 

While the unemployment rate yesterday has a large influence on the unemployment rate today, the unemployment rate last month has only a small influence on the unemployment rate this month. To test this hypothesis, I calculated the mean percent change between observations to be 0.225%. Since the standard deviation is 1.68%, there is most likely some dependence between observations, but this dependence is not too high. Also, Brookings finds that monthly unemployment rates behave similarly to independent random variables, since they can spike or drop at any time^[https://www.brookings.edu/wp-content/uploads/2016/07/2013b_coibion_unemployment_persistence.pdf]. Therefore, I can still conduct my analysis. 

In an optimal world, I would be able to time-average the data over a larger time frame. However, time-averaging would have produced too small of a data frame here. Thus, I proceeded with caution, knowing that the independent and identically distributed or i.i.d. assumption was not 100% met.  

## Data description {-}

**Observations:** The cleaned data set has a total of 732 observations, corresponding to each of the 732 months between January 1960 and December 2020. 

**Response Variable:** Unemployment Rate (UNRATE)^[https://fred.stlouisfed.org/series/UNRATE] is the response variable and is continuous. The unemployment rate represents the number of unemployed as a percentage of the labor force. Labor force data are restricted to people 16 years of age and older, who currently reside in 1 of the 50 states or the District of Columbia, who do not reside in institutions (e.g., penal and mental facilities, homes for the aged), and who are not on active duty in the Armed Forces. The response variable is reported monthly and is seasonally adjusted. 

**Explanatory Variables**

The cleaned data set includes 158 features, and documentation of each feature can be found in Table \@ref(tab:vars-appendix). 156 of the explanatory variables consist of employment-related variables and economic-related variables that are pulled from the employment situation and GDP reports. Additionally, I included both inflation and the federal funds rate as features. All features are continuous.

*Detailed descriptions of added variables:*

*Inflation*: According to economic theory, as unemployment rates fall, the rate of inflation rises. This has been formalized according to what is known as “the Phillips Curve”, which is shown in Figure  \@ref(fig:phillips-curve). 

Inflation (FPCPITOTLZGUSA)^[https://fred.stlouisfed.org/series/FPCPITOTLZGUSA] as measured by the consumer price index reflects the annual percentage change in the cost to the average consumer of acquiring a basket of goods and services that may be fixed or changed at specified intervals, such as yearly. The Laspeyres formula is generally used. This metric is not seasonally adjusted and is recorded annually.

*Federal Funds Rate*: It is thought that the unemployment rate and federal funds rate have a negative contemporaneous relationship. I expect that when the unemployment rate is at its highest,  the federal funds rate will be at its lowest. This likely happens because there is a lower federal funds rate in a weak economy^[https://minds.wisconsin.edu/bitstream/handle/1793/77330/Federal%20Funds%20Rate.pdf?sequence=1&isAllowed=y].

```{r phillips-curve, fig.align='center', fig.cap = "Phillips Curve", out.width="40%", echo = FALSE}
knitr::include_graphics("~/Desktop/STAT471/unemployment-project/results/phillips-curve-model.png")
```

The federal funds rate (FEDFUNDS)^[https://fred.stlouisfed.org/series/FEDFUNDS] is the interest rate at which depository institutions trade federal funds (balances held at Federal Reserve Banks) with each other overnight. When a depository institution has surplus balances in its reserve account, it lends to other banks in need of larger balances. In simpler terms, a bank with excess cash, which is often referred to as liquidity, will lend to another bank that needs to quickly raise liquidity.
 
## Data allocation {-}

To allocate my data, I used an 80-20 split, such that the training dataset consists of 80% of observations and the test data set consists of 20% of observations. Observations were allocated randomly. The same train-test split was used for each class of methods. Additionally, data exploration used solely the train data set. Thus, there are 585 observations in test dataset and 147 observations in the train dataset. 

# Data exporation {-}

## Response Variable {-}

First, I looked at the response variable’s distribution. As seen in the histogram of the unemployment rate variable (Figure \@ref(fig:histogram-response)), the data appears to be right-skewed, with some months having an unemployment rate that exceeds 10%. The median unemployment rate is 5.6%. 

```{r histogram-response, fig.align='center', fig.cap = "Histogram of Unemployment Rate", out.width="60%", echo = FALSE}
knitr::include_graphics("~/Desktop/STAT471/unemployment-project/results/histogram_unemploy.png")
```
Next, I looked at which years have the most extreme unemployment rates and determined that those months corresponded to recessionary periods. Figure \@ref(fig:top-10-unemploy) shows that when unemployment rates are aggregated across each year, the highest unemployment rates occur in 1975-1976, 1981-1984, and 2009-2012, which are all recession years.  

```{r top-10-unemploy, fig.align='center', fig.cap = "Years with the Highest Average Monthly Unemployment", out.width="60%", echo = FALSE}
knitr::include_graphics("~/Desktop/STAT471/unemployment-project/results/top_10_unemploy.pdf")
```
Then, I looked at the mean unemployment rate from 1960-2020 and from 2010-2020, which are reported in Figure \@ref(fig:mean-unemployment-chart). I found that recent unemployment rates are relatively consistent with unemployment rates across the entire time frame. 

```{r mean-unemployment-chart, fig.align='center', fig.cap = "Mean Unemployment Rate by Time Period", out.width="60%", echo = FALSE}
knitr::include_graphics("~/Desktop/STAT471/unemployment-project/results/mean-unemployment-chart.pdf")
```
Finally, I plotted unemployment rate by date for all years (Figure \@ref(fig:all-year-comparison-plot)) to visualize how the unemployment rate has changed over time.  

```{r all-year-comparison-plot, fig.align='center', fig.cap = "Total U.S. Unemployment by Date", out.width="75%", echo = FALSE}
knitr::include_graphics("~/Desktop/STAT471/unemployment-project/results/all-year-comparison-plot.png")
```
## Variation in features  {-}

To do: create some boxplots and insert here 

## Covariation between features {-}

As mentioned above, many of the original features had high covariation since FRED reports include adjusted and unadjusted metrics. After cleaning the data to adjust for multicollinearity issues, no features in the data set have a correlation of 0.9 or above with each other. 

While there are far too many features to create a correlation plot with every feature, I randomly subsampled 30 features to create a correlation matrix. Figure \@ref(fig:corr-plot-subsample) demonstrates how the majority of the features are not highly correlated with each other. Though I do not believe multicollinearity is a major cause for concern after cleaning the data set, penalized regression techniques (e.g. lasso, ridge) will help adjust for possible multicollinearity issues. 

```{r corr-plot-subsample, fig.align='center', fig.cap = "Correlation plot for a subsample of 30 randomly selected variables ", out.width="60%", echo = FALSE}
knitr::include_graphics("~/Desktop/STAT471/unemployment-project/results/corr-plot-subsample.png")
```

Additionally, in Figure \@ref(fig:corr-plot-response), I made a correlation plot that shows the correlation plot between the unemployment rate and all explanatory variables. The plot does not indicate that any variables are strongly positively / negatively correlated with unemployment rate. Therefore, I collect the five features with the hightest absolute correlation with the response variable in Table \@ref(tab:corr-response-chart). 

```{r corr-plot-response, fig.align='center', fig.cap = "Plot of correlations between unemployment rate and all variables",   out.width="100%", echo = FALSE}
knitr::include_graphics("~/Desktop/STAT471/unemployment-project/results/response-corr-plot.png")
```

Table \@ref(tab:corr-response-chart) indicates annual and quarterly depreciation of fixed assets/consumption of fixed capital (A024RL1A225NBEA, A024RL1Q225SBEA) are negatively correlated with the unemployment rate meaning as depreciation increases, the U.S. unemployment rate decreases. Also, as the number of mining and logging employees (CES1000000006) and the number of female mining and logging employees (CES1000000010) increases, so does the U.S. unemployment rate. This potentially suggets that people turn to mining/logging jobs when they cannot find jobs elsewhere. Finally, the average hours of production in the manufacturing sector (AWHMAN) is negatively correlated with unemployment rate, suggesting as manufacturing employees work more hours, the unemployment rate declines. 

```{r corr-response-chart, message = FALSE, echo = FALSE}
#download data sets 
corr_response = read_tsv(file = "~/Desktop/STAT471/unemployment-project/data/clean/corr_response.tsv") %>% 
  arrange(desc(abs(UNRATE))) %>% head(6)  %>% tail(5)

#pull data descrition for selected variables
description = c()
for(i in 1:nrow(corr_response)){
  new_description = var_names %>% filter(id %in% corr_response[i,]$names) %>% 
                        dplyr::select(title) %>% pull()
  description = c(description, new_description)
} 

corr_response = cbind(corr_response, description)
#create a nice table
corr_response %>% #only select top 10
  kable(format = "latex", row.names = NA, 
        booktabs = TRUE, digits = 2, 
        col.names = c("Feature", "Correlation", "Description"),
        caption = "Coefficents with the highest correlation to unemployment rate") %>%
  kable_styling(position = "left", font_size = 8) %>%
  column_spec(3, width = "10cm")
```

## Initial Insights: Employment Situation Features {-}

### Number of Employees by Industry

Next, I wanted to see how the number of employees across sectors is correlated with the unemployment rate Specifically, I looked at the logging, shipping/boating, information, and federal sectors. Due to the recent increase in the number of information jobs, I also plotted information vs. unemployment for 2000-2020. Figure \@ref(fig:employees-plot) suggests that as the number of employees declines in any industry, the unemployment rate rises. However, the number of federal employees stays relatively constant despite changes in the unemployment rate. It is important to note that changes in the number of employees per industry might not be due to changes in the unemployment rate. Instead, demand for workers across sectors might have shifted during the time frame of the dataset. 

```{r employees-plot, fig.align='center',  fig.cap = "Unemployment Rate by Number of Employees by Industry", out.width="65%", echo = FALSE}
knitr::include_graphics("~/Desktop/STAT471/unemployment-project/results/employees_plot.png")
```

### Average Weekly Hours of Production by Industry

I would predict that as the average number of hours of production increases, the unemployment rate decreases. I looked at the manufacturing, mining/logging, and construction industries. Figure \@ref(fig:hours-plot) shows that my prediction holds. Out of selected variables, the manufacturing industry has the strongest correlation between an increase in average weekly hours of production and a decrease in the unemployment rate. 

```{r hours-plot, fig.align='center',  fig.cap = "Unemployment Rate by Average Weekly Hours of Production by Industry", out.width="70%", echo = FALSE}
knitr::include_graphics("~/Desktop/STAT471/unemployment-project/results/hours_production_plot.png")
```

## Initial Insights: Gross Domestic Product Summary Features {-}

I also examined features that were present in the GDP report. I chose to plot four variables: social benefits to persons, net government saving, real consumption of fixed capital, and real disposable personal income in (Figure \@ref(fig:gdp-plots)). Unemployment rate increases when government social benefits increase, which likely occurs because unemployment is a qualifying factor for many of these benefits. Also, as real consumption of fixed capital and real disposable personal income increases, the unemployment rate generally falls. This makes sense because corporations are willing to invest more in fixed assets (e.g. buildings) when the economy is doing well. Additionally, disposable personal income increases when someone is employed and when economic growth is strong. 

```{r gdp-plots, fig.align='center',  fig.cap = "Unemployment Rate by Variables from GDP Report", out.width="70%", echo = FALSE}
knitr::include_graphics("~/Desktop/STAT471/unemployment-project/results/gdp_plots.png")
```

## Analysis of Inflation and Federal Funds Rate {-}

### Inflation

I would expect inflation to rise as the unemployment rate decreases due to the Phillips Curve (Figure \@ref(fig:phillips-curve)). However, as the unemployment rate falls, inflation does not appear to rise. There is no apparent relationship between these two variables. 

```{r phillips-curve-us-plot, fig.align='center', fig.cap = "Inflation vs. Unemployment Rate. Replicates the Phillips Curve for dataset.", out.width="50%", echo = FALSE}
knitr::include_graphics("~/Desktop/STAT471/unemployment-project/results/phillips-curve-us-plot.png")
```

### Federal Funds

Next, I examined whether the unemployment rate is negatively correlated with the federal funds rate. I only looked at the last 20 years because line plots with all 585 observations can be hard to interpret due to a few outliers. Figure \@ref(fig:fed-funds-plot) implies a direct, negative linear relationship between the federal funds rate and the unemployment rate. Thus, Figure \@ref(fig:fed-funds-plot) implies when the unemployment rate is high, the federal funds rate will be low and vice versa.

```{r fed-funds-plot, fig.align='center', fig.cap = "Unemploy by Federal Funds Rate", out.width="40%", echo = FALSE}
knitr::include_graphics("~/Desktop/STAT471/unemployment-project/results/fed-funds-plot.png")
```

# Modeling {-}

# Modeling Class 1: Regression Methods {-}

## Ordinary least squares {-}

To start off, I created a ordinary least squares model that included all explanatory variables. Based on the regression summaruym, the ordinary least squares model has an r-squared value of 0.991, which implies that the features explain about 99.1% of the variation in the unemployment rate. Additonally, there are 34 statisically significant features in the model at a 0.05 threshold. (Table \@ref(tab:lm-coefs)) shows the features with the lowest p-values.

```{r lm-coefs,  message = FALSE, echo = FALSE}
lm_features = read_tsv("~/Desktop/STAT471/unemployment-project/results/lm-sig.tsv") %>% 
  rename(feature = "rownames(lm_sig)", "p-value" = sig) %>% 
  filter(!(feature == "(Intercept)")) %>%
head(10)

description = c()
for(i in 1:nrow(lm_features)){
  new_description = var_names %>% filter(id %in% lm_features[i,]$feature) %>% 
                        dplyr::select(title) %>% pull()
  description = c(description, new_description)
} 
lm_features = cbind(lm_features, description)

lm_features %>% #only select top 10
  kable(format = "latex", row.names = NA, 
        booktabs = TRUE, digits = 8, 
        col.names = c("Feature", "p-value", "Description"),
        caption = "Coefficents with the lowest p-value for ordinary least squares model") %>%
  kable_styling(position = "left", font_size = 8) %>%
  column_spec(3, width = "10cm")
```
Next, I check to make sure the necessary assumptions for linear regression are meet in Figure \@ref(fig:lm-assumptions). First, I checked the linearity of the data in the Residuals vs Fitted plot. Though there are a few outliars, the residual plot shows no fitted pattern, and the red line is approximately horizontal at zero. Next, I check the homogeneity of residuals variance by looking at the Scale-Location plot. This plot shows that residuals are spread equally along the ranges of predictors. The QQ plot of residuals is used to check the normality assumption. The normal probability plot of residuals should approximately follow a straight line. The normal probability plot of residuals approximately follows a straight line. Finally, I checked to see if the residuals have a constant variance in the Residuals vs. Leverage plot. Since there are only 2 extreme outliars, it is okay to assume this assumption is met. 

```{r lm-assumptions, out.width = "60%", fig.cap = "Check assumptions for ordinary least squares model ", fig.align='center', echo = FALSE}
knitr::include_graphics("~/Desktop/STAT471/unemployment-project/results/lm_assumptions.png")
```

While the ordinary least squares method seems to work well, training a model with such few observations on so many explanatory variables might lead to overfitting. Due to a high r-squared value, I suspect that bias is high but variance is low in the ordinary least squares model. Therefore, I decided to build and evaluate shrinkage models with the hopes of getting a more interpretable and accurate model. I ran three cross-validated regressions for which optimal values of lambda were chosen according to the one-standard-error rule: ridge, LASSO (Least Absolute Shrinkage and Selection Operator), and elastic net. 

## Ridge Regression {-}

Next, I fit  a 10-fold cross-validated ridge regression model to the training data. In (Figure \@ref(fig:ridge-cv-plot)) we have the CV plot for the model. Corresponding to the right vertical dashed line on the plot (on the log scale), the value of lambda selected according to the one-standard-error rule is about 0.111. 

```{r ridge-cv-plot, out.width = "60%", fig.cap = "Ridge CV plot: This is the CV plot for the 10-fold cross-validated ridge regression model on the training data", fig.align='center', echo = FALSE}
knitr::include_graphics("~/Desktop/STAT471/unemployment-project/results/ridge-cv-plot.png")
```

Next, I used plot_glmnet to visualize the ridge regression fitted coefficients, highlighting the 6 features with the highest absolute standardized coefficents. It appears that none of the coefficents change signs as lambda incresaes. 

(Table \@ref(tab:ridge-coefficients)) shows the features that have the highest standardized coefficents and provides their descriptions. 

```{r ridge-trace-plot, out.width = "60%", fig.cap = "Ridge trace plot with 6 features with highest magnitude highlighted", fig.align='center', echo = FALSE}
knitr::include_graphics("~/Desktop/STAT471/unemployment-project/results/ridge_coef_plot.png")
```

Four of the features descripe the number of employees by sector. When the number of goods producing (CES0600000006) and construction employees (CES2000000006) increases, unemployment falls. However, when the number of mining/logging, for all employees (CES1000000006) and female employees (CES1000000010), increases, the unemployment rate rises. This might imply that people are more willing to enter the mining/logging field when it is difficult for them to find a job in another sector. Likewise, as number of manufacturing overtime hours(A2224C1A027NBEA) increases, unemployment decreases, likely because overtime hours signal a tight labor market. 

Additionally, as balance on current accounts (A1610C1A027NBEA) increases, unemployment rate increases. The current account balance of payments is a record of a country's international transactions with the rest of the world ^[https://data.oecd.org/trade/current-account-balance.htm]. Perhaps, as the United States trades more with foreign countries, there is less of a demand for jobs domestically, which drives up the unemployment rate. 

Three features are signals of the state of the economy. As corporate dividends (A2224C1A027NBEA) and rental income (A048RC1A027NBEA) increase, unemployment rate decreases. As the amount of government social benefits (A1589C1A027NBEA) inreases, unemployment rate increases. In times of high unemployment, corporations have increased retained earnings to pay out as dividends and individuals have more money to spend on properties. In contrast, in times of high unemployment, individuals rely more on social programs because more individudals can claim unemploymnent insurance. 

```{r ridge-coefficients, message = FALSE, echo = FALSE}
ridge_features = read_tsv("~/Desktop/STAT471/unemployment-project/results/ridge-features-table.tsv") %>% 
head(10)

description = c()
for(i in 1:nrow(ridge_features)){
  new_description = var_names %>% filter(id %in% ridge_features[i,]$feature) %>% 
                        dplyr::select(title) %>% pull()
  description = c(description, new_description)
} 

ridge_features = cbind(ridge_features, description)

ridge_features %>% #only select top 10
  kable(format = "latex", row.names = NA, 
        booktabs = TRUE, digits = 2, 
        col.names = c("Feature", "Coefficient", "Description"),
        caption = "Standardized coefficients for features in the ridge 
        model based on the one-standard-error rule.") %>%
  kable_styling(position = "left", font_size = 8) %>%
  column_spec(3, width = "10cm")
```

## Lasso Regression {-}

Next, I fit a 10-fold cross-validated lasso regression to the training data. In (Figure \@ref(fig:lasso-cv-plot)), we have the CV plot for the model. Corresponding to the right vertical dashed line on the plot (on the log scale), the value of lambda selected according to the one-standard-error rule is about 0.00316. 80 features (excluding the intercept) are selected if lambda is chosen according to the one- standard-error rule. 

(Figure \@ref(fig:lasso-trace-plot)) shows the lasso trace plot with the first 6 features to enter the model highlighted and (Table \@ref(tab:lasso-coefficients)) shows the top 10 selected features with the highest standardized coefficents. 

```{r lasso-cv-plot, out.width = "60%", fig.cap = "Lasso CV plot: This is the CV plot for the 10-fold cross-validated lasso regression model on the training data.", fig.align='center', echo = FALSE}
knitr::include_graphics("~/Desktop/STAT471/unemployment-project/results/lasso-cv-plot.png")
```

Examining this lasso trace plot in (Figure \@ref(fig:lasso-trace-plot)), we see that lambda decreases from left to right. Thus, Real Consumption of Fixed Capital: Private (A024RL1A225NBEA) is the first feature to enter the model as lambda decreases, and the coefficent is negative.  Other features with negative standardized that enter in the first 6 are: Real Gross Private Domestic Investment: Fixed Investment: Nonresidential: Structures (A009RO1Q156NBE) and Average Weekly Hours of Production and Nonsupervisory Employees,Manufacturing (AWHMAN). Features with positive standardized coefficents include: Net lending or net borrowing (-), NIPAs: Government: Statisticaldiscrepancy (A030RC1Q027SBE), Production and Nonsupervisory Employees, Goods-Producing (CES1000000006), and Women Employees, Mining and Logging (CES1000000010). 

```{r lasso-trace-plot, out.width = "60%", fig.cap = "Lasso trace plot with first 6 features to enter the model highlighted", fig.align='center', echo = FALSE}
knitr::include_graphics("~/Desktop/STAT471/unemployment-project/results/lasso_coef_plot.png")
```

(Table \@ref(tab:lasso-coefficients)) shows the features with the highest standardized coefficents. 

Many of the coefficents for the LASSO model and consistent with the ridge model. Three of the features descripe the number of employees by sector. The lasso model finds that as the number of goods producing (CES0600000006) and construction employees (CES2000000006) increases, unemployment falls. Also, when the number of mining/logging for all employees (CES1000000006) increases, the unemployment rate rises. As rental income (A048RC1A027NBEA) increases, unemployment rate decreases. As gross domestic product: farm products consumed on farms (A2051C1A027NBEA) increases, so does unemplopument rate. Finally, as the amount of government social benefits (A1589C1A027NBEA) inreases, unemployment rate increases. 

However, lasso has four different features in the top 10 standardized coefficent list.  As taxes on corporate income (A054RE1A156NBEA) increase, unemployment rate decreases. This likely occurs because the taxes are paid on net income, which is higher when the economy is doing well. Thus, when corporate profits are higher, corporate taxes are higher. Next, as the share of GDP made up by exports of goods (A253RE1A156NBEA, A019RE1A156NBEA) increases, the unemployment rate increases. At first, this relationship seems counterintuitive. While creating more goods in the U.S. should increase demand for labor, exports of goods typically peak right before a recession^[https://fred.stlouisfed.org/series/A253RE1A156NBEA]. Thus, having a high percentage of GDP made up by the exportation of goods could signal that the United States is about to enter a recession. However, this is uncertainl. 

Finally, as levels of personal savings (A2122C1A027NBEA) increase, unemployment declines. When more people have jobs, they have more money to spend and more money to save. 

```{r lasso-coefficients, message = FALSE, echo = FALSE}
lasso_features = read_tsv("~/Desktop/STAT471/unemployment-project/results/lasso-features-table.tsv") %>% 
head(10)

description = c()
for(i in 1:nrow(lasso_features)){
  new_description = var_names %>% filter(id %in% lasso_features[i,]$feature) %>% 
                        dplyr::select(title) %>% pull()
  description = c(description, new_description)
} 

lasso_features = cbind(lasso_features, description)

lasso_features %>% #only select top 10
  kable(format = "latex", row.names = NA, 
        booktabs = TRUE, digits = 2, 
        col.names = c("Feature", "Coefficient", "Description"),
        caption = "Standardized coefficients for features in the lasso
        model based on the one-standard-error rule.") %>%
  kable_styling(position = "left", font_size = 8) %>%
  column_spec(3, width = "10cm")
```

## Elastic Net  Regression {-}

The last regression method I used was the elastic net regression, which combines the penalties from ridge and LASSO regression techniques. When alpha = 0, a ridge regression is produced. When alpha = 1, a LASSO regression is produced. In an elastic net, 0 < alpha < 1, which creates a ridge-like shrinkage as well as lasso-like selection. As long as alpha is greater than 0, shrinkage will occur. 

To choose alpha, I cross-validated over alpha and lambda. First, I choose alpha to minimize CV error, and then I selected lambda according to the one-standard-error rule. (Figure \@ref(fig:elastic-net-CV-plot)) shows CV error for each value of alpha. Alpha is set to 0.343, corresponding to the vertical dashed line on the plot, and lambda selected according to the one-standard-error rule is about 0.00579. 

```{r elastic-net-CV-plot, out.width = "50%", fig.cap = "Elastic Net CV plot", fig.align='center', echo = FALSE}
knitr::include_graphics("~/Desktop/STAT471/unemployment-project/results/cv_elnet_plot.png")
```

```{r elnet-trace-plot, out.width = "60%", fig.cap = "Elastic net trace plot with only first 6 enter the model highlighted", fig.align='center', echo = FALSE}
knitr::include_graphics("~/Desktop/STAT471/unemployment-project/results/elnet_coef_plot.png")
```

Examining this elastic net trace plot in (Figure \@ref(fig:elnet-trace-plot)), Real Consumption of Fixed Capital: Private (A024RL1A225NBEA) is the first feature to enter the model as lambda decreases, and the coefficent is negative.  

Similar to the LASSO trace plot,, other features with negative standardized that enter in the first 6 are: Real Gross Private Domestic Investment: Fixed Investment: Nonresidential: Structures (A009RO1Q156NBE) and Average Weekly Hours of Production and Nonsupervisory Employees,Manufacturing (AWHMAN). In constrast to LASSO, the elastic net also includes eal Consumption of Fixed Capital: Private measured on a quarterly basis (A024RL1Q225SBE). 

Features with positive standardized coefficents include: Production and Nonsupervisory Employees, Goods-Producing (CES1000000006) and Women Employees, Mining and Logging (CES1000000010). Both of these features were also found in the LASSO trace plot. 

```{r elastic-net-coefficients, message = FALSE, echo = FALSE}
elnet_features = read_tsv("~/Desktop/STAT471/unemployment-project/results/elnet-features-table.tsv")  %>% 
head(10)

description = c()
for(i in 1:nrow(elnet_features)){
  new_description = var_names %>% filter(id %in% elnet_features[i,]$feature) %>% 
                        dplyr::select(title) %>% pull()
  description = c(description, new_description)
} 

elnet_features = cbind(elnet_features, description)

elnet_features %>% #only select top 10
  kable(format = "latex", row.names = NA, 
        booktabs = TRUE, digits = 2, 
        col.names = c("Feature", "Coefficient", "Description"),
        caption = "Standardized coefficients for features in the elastic net
        model based on the one-standard-error rule.") %>%
  kable_styling(position = "left", font_size = 8) %>%
  column_spec(3, width = "10cm")
```

Table \@ref(tab:lasso-coefficients) shows the features with the highest standardized coefficents. 

Given that alpha is set to 0.343, I would expect similar coefficents to show up in the elastic net as the ridge regression. Seven of the same features are present in the elastic net regression that are present in the ridge regression. 

Across all three models, the number of goods producing (CES0600000006) and construction employees (CES2000000006) increases, unemployment falls. However, when the number of mining/logging (CES1000000006), increases, the unemployment rate rises. In all three models, as rental income (A048RC1A027NBEA), government social benefits (A1589C1A027NBEA), and farm products consumed on farms (A2051C1A027NBEA) increase, unemployment rate decreases. 

Like ridge, the elastic net finds that as balance on current accounts (A1610C1A027NBEA) increases, unemployment rate increases. Like LASSO, the elastic net finds that as the corporate income tax (A054RE1A156NBEA) increases and as the share of GDP made up by exports of goods (A253RE1A156NBEA, A019RE1A156NBEA) increases, unemployment rate increases. 

# Modeling Class 2: Tree-based Methods {-}

Next, I will predict unemployment using tree based methods.  

While I did not use a simple decision tree to predict unemployment rate, (Figure \@ref(fig:basic-tree-plot)) visualizes what a decision tree looks like for the data set. Decision trees partition the feature space into axis-aligned nested rectangles, producing a constant prediction for feature vectors in each rectangle. Behind the scenes, the random forest model and boosting model create trees similar to the one pictured.

```{r basic-tree-plot, out.width = "50%", fig.cap = "", fig.align='center', echo = FALSE}
knitr::include_graphics("~/Desktop/STAT471/unemployment-project/results/decision-tree-plot.png")
```

## Random Forest {-}

When it comes to prediction accuracy, simple decision trees suffer because of their high variance. Random forests attempt to reduce variance while keeping bias around the same by averaging many trees together. At each split point of each tree, a random subset of features is selected and the tree is split on the best feature among the subset. If the number of features selected at each split is larger, the random forest will have lower bias (it can better fit the underlying trend) but higher variance (more correlated trees). Therefore, I tuned the random forest based on the number of variables randomly selected at each split (mtry). 

Since there are 158 (excluding UNRATE) variables, mtry can range from 1 to 159, and is set to a deault value of 12, which the square root of the number of variables. (Figure \@ref(fig:rf-mtry-plot)) shows the out of bag error (OOB error) as a function of mtry. The best value of mtry according to (Figure \@ref(fig:rf-mtry-plot)) is 17. 

```{r rf-mtry-plot, out.width = "60%", fig.cap = "OOB error as a function of mtry", fig.align='center', echo = FALSE}
knitr::include_graphics("~/Desktop/STAT471/unemployment-project/results/mtry_plot_rf.png")
```

Next, I checked to see at what number of trees does OOB stablize at. (Figure \@ref(fig:rf-mtry-plot)) plots OOB error as a function of the number of trees. (Figure \@ref(fig:rf-mtry-plot)) shows that OBB error stablizes at around 50 trees. While the out of bag eror appears to have flattened out by the time 50 trees are trained, I trained of 300 trees just to make sure. 

```{r num-trees-check, out.width = "60%", fig.cap = "OBB error as a function of the number of trees", fig.align='center', echo = FALSE}
knitr::include_graphics("~/Desktop/STAT471/unemployment-project/results/OBB_error_num_tree_rf.png")
```

**The tuned random forest model is trained using 300 trees and 17 random variables sampled from at each split**

Next, I determined which variables were most important in the random forest. Compared to trees, main drawback of random forests is reduced interpretability. However, I used variable importance measures to improve the interpretability. 

Two types of variable importance measures are used for random forests:
> Purity based importance: how much improvement in node purity results from splitting on a feature

> OOB prediction based importance: how much deterioration in prediction accuracy results from scrambling a feature out of bag

(Figure \@ref(fig:var-imp-plot)) shows the features with the top variable importance measures, using both types of measures. For the purposes of this report, I will only look at OOB prediction based importance, which is labeled %IncMSE in (Figure \@ref(fig:var-imp-plot)). 


```{r var-imp-plot, out.width = "90%", fig.cap = "Variable Importance", fig.align='center', echo = FALSE}
knitr::include_graphics("~/Desktop/STAT471/unemployment-project/results/var-imp-rf.png")
```

The ten variables with the highest variable importance are found in Table \@ref(tab:rel-imp). However, due to the limitations of the random forest model, how exactly these variables impact unemployment rate is unknown. 

```{r rel-imp, message = FALSE, echo = FALSE}
rf_features = read_tsv("~/Desktop/STAT471/unemployment-project/results/var_imp_rf.tsv") %>% 
  rename(var = "rownames(var_imp_rf)", imp = "varImpPlot.rf_fit..n.var...10....1.") %>% 
  arrange(desc(imp)) %>% 
  head(10)  

description = c()
for(i in 1:nrow(rf_features)){
  new_description = var_names %>% filter(id %in% rf_features [i,]$var) %>% 
                        dplyr::select(title) %>% pull()
  description = c(description, new_description)
} 

rf_features  = cbind(rf_features , description)

rf_features  %>%  
  kable(format = "latex", row.names = NA, 
        booktabs = TRUE, digits = 2, 
        col.names = c("Feature", "Relative Importance", "Description"),
        caption = "Top 10 relative importance for random forest model") %>%
  kable_styling(position = "left", font_size = 8) %>%
  column_spec(3, width = "10cm")
```

## Boosting {-}

While random forests grow deep decision trees in parallel, bossting grows shallow decision trees sequentially. Therefore, I need to consider how deep to grow my trees (interaction depth), how slow the boosting model should learn, and how many trees to grow. 

First, I fit boosted tree models with interaction depths of 1, 2, and 3. For each tree model, I used a shrinkage factor of 0.1, 1000 trees, and 5-fold cross validation. (Figure \@ref(fig:gbm-cv-plot)) shows the cross validation error plots for each tree with different interaction depths. The blue line on (Figure \@ref(fig:gbm-cv-plot)) has the lowest crossvalidation error, so the optimal interaction depth is 3. 

Next, I looked for the optimal number of trees to grow. (Figure \@ref(fig:gbm-optimal-trees)) shows that the optimal number of trees to grow is 990. As (Figure \@ref(fig:gbm-optimal-trees)), if I choose a smaller number of trees, error would not increase dramatically. Finally, I left the subsampling fration equal to 0.5. 

```{r gbm-cv-plot, out.width = "60%", fig.cap = "CV error plot for boosted tree model with interaction depth 1, 2, and 3", fig.align='center', echo = FALSE}
knitr::include_graphics("~/Desktop/STAT471/unemployment-project/results/gmb_cv_error.png")
```

```{r gbm-optimal-trees, out.width = "60%", fig.cap = "Squard error loss as a function of the number of trees, where blue dashed line represents optimal number of trees", fig.align='center', echo = FALSE}
knitr::include_graphics("~/Desktop/STAT471/unemployment-project/results/gbm-optimal-trees.png")
```

```{r boosting-features, message = FALSE, echo = FALSE}
boosting_features = read_tsv("~/Desktop/STAT471/unemployment-project/results/gbm-summary.tsv") %>% 
  head(10)

description = c()
for(i in 1:nrow(boosting_features)){
  new_description = var_names %>% filter(id %in% boosting_features[i,]$var) %>% 
                        dplyr::select(title) %>% pull()
  description = c(description, new_description)
} 

boosting_features = cbind(boosting_features, description)

boosting_features %>%  
  kable(format = "latex", row.names = NA, 
        booktabs = TRUE, digits = 2, 
        col.names = c("Feature", "Relative Importance", "Description"),
        caption = "Top 10 relative importance for boosting model") %>%
  kable_styling(position = "left", font_size = 8) %>%
  column_spec(3, width = "10cm")
```


```{r pdp-plots, out.width = "70%", fig.cap = "Partial dependence plots for top ten features based on relative influence for optimal boosting model", fig.align='center', echo = FALSE}
knitr::include_graphics("~/Desktop/STAT471/unemployment-project/results/pdp_plots.png")
```

To visualize the effects of the top ten features based on relative influence for optimal boosting model, I used partial dependence plots (Figure \@ref(fig:pdp-plots)). 

The percent change of Real Consumption of Fixed Capital: Private (A024RL1A225NBEA) is the feature with the highest relative importance. As shown in  Table \@ref(tab:corr-response-chart), this feature has the highest absolute correlaton with the unemployment rate. Consumption of fixed capital is a term used to describe the depreciation of fixed assets. As companies invets more in fixed assets, deprecition of these assets increases due to a higher depreciation base. As seen in Figure \@ref(fig:pdp-plots), as consumption of fixed capital increases, unemployment rate declines. This happens because companies are more likely to invest in buildings and other fixed assets when they are doing well. 

Two of the features descripe the number of employees by sector. Unsurprisingly, the number of mining employees (CES1000000006) is the second most important feature. Like the regression methods found, as the number of mining employees increases, unemployment rate also increase. However, this pattern is only observed after the number of mining employees reaches 550 thousand employees.Before reaching 550 thousands, the unemployment rate falls as the number of mining employees increases. The number of manufacturing employees (CES3000000006) shows up as a significant feature in both the random forest and boosting models, but is not found in the regression models. As the number of manufactering employees increases, unemployment rate decreases. Unemployment rate levels off after the number of manufacturing employees reaches 8.5 million. 

Fixed investment in residental as a percentage of GDP (A011RE1A156NBEA) is found in both the random forest and boosting models. As fixed investment in residental as a percentage of GDP increases, unemployment rate decreases. This likely happens because real estate developers expect increased demand for housing (apartments, single-family homes, etc.) during good economic times. 
 
Farm output (A365RG3A086NBEA) is also predictive unemployment rates. As farm output increases, unemployment rates increases. While it is hard to justify why this behavior occurs, it is possible that working on a farm is undesirable when unemployment rates are low. Therefore, operating or working on a farm is more desirable when people cannot find jobs elsewhere. 

Annaul net lending or net borrowing (-), NIPAs: Government (A030RC1A027NBEA). As net lending increases, unemployment rate increases. When unemployment rates are high, the government has to take on more debt to pay for social programs and experiences lower tax revenues. Likewise,  te variable importnace for unemployment insurance (A1589C1A027NBEA) is also high. As unemployment insurance increases, unemployment rates also increase. The ridge, lasso, elastic net, and random forest models all include unemployment insurance as a feature. 

Three factors relate to private domestic investments: Change in private inventories (A014RE1A156NBEA), private investment in nonresidential structures (A009RO1Q156NBEA), and gross private direct investment (A006RE1A156NBEA). 

Change in private inventories describes the increase or decrease in the stocks of final goods, intermediate goods, raw materials, and other inputs that businesses keep on hand to use in production. \@ref(fig:pdp-plots) shows that when the change in private inventories is negative, unemployment rate higher than when the change in private inventories is positive. Lower inventories and raw materials on hand likely signals that a company expects lower sales or perform worse in the future. When a company expects to perform poorly, they are likely to lay off workers. Similarly, as private investment in nonresidential structures and gross private direct investment increases, the unemployment rate falls, since corporations and individuals have more money to invest during periods of economic growth. 

# Conclusions {-}

## Method comparison {-}

To determine which model performed the best, I calculated test root mean sqaured error for each model. The lower the RMSE, the better the model is able to fit the data set. To provide a baseline, I also calculated the RMSE of an intercept-only model, where unemployment rate is predicted to be the mean unemployment rate of the training data set. Table \@ref(tab:test-RMSE) shows the test RMSE for each model. The intercept-only model, is the simplest possible model and has high bias and low variance. 

Tree-based methods were more predictive than regression methods. I found that boosting model performs marginally better than random forest model. The RMSE for the boosting model is 0.882, which means that the square root of the variance of the residuals is 0.882. Thus, on average, the boosting model predicts unemployment rate to be plus or minus 0.882% of the true unemployment rate for the month. 

Looking at regression methods, the least squares regression performs marginally better than all three penalized regression methods. The least squares, ridge, LASSO, and elastic net regressions all perform better than the intercept only model. Since the LASSO model performs only slightly worse than the least squares model, I believe the LASSO model is the best out of all of the regression models. Since it has 80 variables, instead of 159 it is easier to interpret. 

```{r test-RMSE, message = FALSE, echo = FALSE}
read_tsv("~/Desktop/STAT471/unemployment-project/results/error_for_models.tsv") %>% 
  kable(format = "latex", row.names = NA, 
                           booktabs = TRUE,
                           digits = 3,
                           col.names = c("Model type", 
                                         "Root mean squared error"),
        caption = "RMSE by Model Type") %>%
  kable_styling(position = "center") %>%
  kable_styling(latex_options = "HOLD_position") 
```

The boosted model performs the best because it is better able to capture the underlying trends in the data. It is likely that the underlying trend in unemployment is not linear, since the unemployment rate normally does not dip below the natural rate of unemployment. Also, the Fed's dual mandate helps ensure that unemployment does not exceed a certain percentage. Therefore, recursively partitioning the feature space better represents the underlying trends, which decreases bias. Additionally, since booosting agrregates multiple decision trees together, variance is reduced and prediction perfomance increaes compared to a decision tree. 

## Takeaways for Stakeholders {-}

While there were difference in test RMSE across models, the methods overlap significantly in their identification of important variables from the larger data set. These coefficents also have the same directional effect in all modeels. Based on economic theory, inflation and the federal funds rate should be predicitve of unemployment. However, these features were not in the top 10 features list for any of the models. 

After examining which features show up in the top 10 feature list across models, I found that 15 features show up more than twice across the penalized regression models (ridge, LASSO, and elastic net) and the tree-based methods models (random forest and boosting). This coefficents can be found in Table \@ref(tab:features-across). While I believe these features are the most predictive of unemployment rate, it is important to note that these feautres are merely correlated with unemployment, and do not necessarily cause unemployment rates to rise or fall. 

```{r features-across, message = FALSE, echo = FALSE}
f_ridge = ridge_features %>% dplyr::select(feature) %>% pull
f_lasso = lasso_features %>% dplyr::select(feature) %>% pull
f_elnet= elnet_features %>% dplyr::select(feature) %>% pull
f_rf = rf_features %>% dplyr::select(var) %>% pull()
f_boost = boosting_features %>% dplyr::select(var) %>% pull

f_all = c(f_ridge, f_lasso, f_elnet, f_boost, f_rf)
f_count = tibble(f_all) %>%
  group_by(f_all) %>% summarise(n = n()) %>% 
  arrange(desc(n)) %>% rename(Variable = f_all, Count = n)

f_count %>% filter(Count > 1) %>%
  mutate(Ridge = (Variable %in% f_ridge), 
         Lasso = (Variable %in% f_lasso), 
         "Elastic Net" = (Variable %in% f_elnet), 
         "Random Forest" = (Variable %in% f_rf), 
         Boosting = (Variable %in% f_boost) )  %>% 
  kable(format = "latex", row.names = NA, 
                           booktabs = TRUE,
                           digits = 3,
        caption = "Features Found in Top 10 Across Models") %>%
  kable_styling(position = "center") %>%
  kable_styling(latex_options = "HOLD_position") 



temp = f_count %>% filter(Count > 1) %>%
  mutate(Ridge = (Variable %in% f_ridge), 
         Lasso = (Variable %in% f_lasso), 
         "Elastic Net" = (Variable %in% f_elnet), 
         "Random Forest" = (Variable %in% f_rf), 
         Boosting = (Variable %in% f_boost) ) %>% dplyr::select(Variable) %>% pull()

description = c()
for(i in 1:length(temp)){
  new_description = var_names %>% filter(id %in% temp[i]) %>% 
                        dplyr::select(title) %>% pull()
  description = c(description, new_description)
} 
```

**(1) Changes in the number of employees for "blue collar" professions are more predcitive than "white collar" professions.**

*Negative effect:*

> (1) Goods-Producing Employees (CES0600000006):  

> (2) Construction Employees (CES2000000006):  

> (3) Manufacturing Employees (CES3000000006) 

*Positive effect:*

> (1) Mining and Logging Employees (CES1000000006):  

As the number of good-producing, construction, and manufacturing employees increases, the unemployment rate falls. This likely occurs because demand for workers in these industries moves closely with economic cycles. In contrast, the unemployment rate increases as the number of mining and logging employees increases. This is likely because more people are willing to work in mining and logging when it is hard to find a job elsewhere. While "blue-collar" professions are highly predictive of unemployment, "white-collar" professions, such as non-goods producing, information, and service employees, are not as predicitive of the unemployment rate. 

**(2) Government spending and government debt increases as a result of high unemployment**

*Positive effect:*

> (1) Unemployment insurance (A1589C1A027NBEA)

> (2) Government net lending or net borrowing (A030RC1A027NBEA)

When unemployment insurance increases and when the government borrows more money, unemployment rate increases. It is logical that more people file for unemployment insurance when more individuals are unemployed. Additionally, to pay for these social benefits, the government has to increase spending, which might increase borrowing. Borrowing might also be higher when unemployment rate is low because the government collects less money in taxes during periods of high unemployment. Firstly, individuals pay less income taxes. Second, corporations typically decrease the number of employees they hire when they are performally poorly. Therefore, the government likely collects less corporate taxes during periods of high unemployment.  

**(3) If the United States exports more goods than it imports, unemployment will fall.**

*Positive effect*

> (1) Percent of GDP made up by exportation of goods (A253RE1A156NBEA) 

> (2) Adjustment to exports for U.S. territories and Puerto Rico  (Balance on Current Accounts) (A1610C1A027NBEA)

*Negative effect*

> (1) Percent of GDP made up by net exports of goods and services (A019RE1A156NBEA)  

While increased exportation of goods is associated with higher unemployment, the net exportation of of good and services is associated with lower unemployment. The reason for this difference stems from the word "net". "Net" implies the difference between U.S. exports of goods and services and U.S. imports of goods and services, and this metric is positive when exports are greater than imports. Therefore, when the U.S. increases exports relative to imports, this signals that more jobs are being created in the United States. However, when the percent of GDP made up by the exportation of goods increases, the amount of goods being imported is most likely increasing at a faster rate, since the U.S. trade deficit has continued to widen since 1960 ^[https://fred.stlouisfed.org/series/A1610C1A027NBEA].

The adjustment to exports for U.S. territories and Puerto Rico is likely found in the model because U.S. territories are not included in the unemployment rate. Therefore, if more goods are exported from territories, fewer goods with be exported from the mainland United States. 

**(4) As more money is invested in fixed assets, the unemployment rates declines **

*Negative effect:*

> Percent of GDP made up by gross private domestic investment in residential (A011RE1A156NBEA)

> Consumption of fixed capital/ depreciation of fixed assets (A024RL1A225NBEA)

Investors and corporations are more likely to invet in fixed / illiquid assets when they believe they are a growing or steady state of their business cycle. When money is invested into residental properities, the developer believes the real estate market is growing and the economy is strong. Likewise, depreciation rises when more money as companies' depreciable base increases. Companies only buy more fixed assets when they believe they are likely to expand and be able to support these assets 

**(5) As corporation profits increase, the unemployment rate falls.**

*Negative effect:*

> Taxes on corporate income  (A054RE1A156NBEA)

> Rental income of persons (A048RC1A027NBEA)

When net income of corporations is higher, they are more likely to hire more employees and not lay off existing ones. Corporations have higher income when they pay higher taxes. Addiitonally, landlords are likely to receive higher rent when more people have jobs and are able to afford rent. Thus, when corporate income and rental income of landloard increase, unemployment rate falls. 

**Potential takeaway: increases in farm output correspond to increase in unemployment since farm output is linked to technological innovation**

*Positive effect:*

> (1) Farm output (A365RG3A086NBEA)

> (2) Farm products consumed on farms (A2051C1A027NBEA)

It is hard to explain why unemployment rate increases when farm output increases and when the amount of farm products consumed on farms increases. One possible explanation for this is that increases in farm output are associated with increased automation ^[https://www.asme.org/topics-resources/content/automating-the-risk-out-of-farming]. I would have to do more research to be certain about this conclusion. 

## Limitations {-}

### Dataset limitations

There are four major limitations of my analysis:

(1) NA values removed 

Orginally, my dataset had 2004 variables and 804 observations. However, not all variables have been reported since 1954 and many variables have not yet been reported for 2021. Therefore, removed all observations before 1960 and after 2020. Also, even though I imputed the values for quarterly and yearly data, I had to remove many columns because mnay R packages require that no NA values are present in the data. It is possibly that variables that started being reported after 1960 are more predictive of unemployment rate than the variables in my model. 
 
(2) Only focused on two monthly releases 

For this report, I focused on two monthly releases: the Unemployment Situation Summary and the Gross Domestic Product Summary. However, it possible that other monthly releases include variables that are more predicitive of unemployment rate. The results of the analysis might change dramatically if I were to incorporate other variables found in other releases. However, due to limited computing power and the fact that the FRED API can only process 120 request/minute, I was not able to examine more variables. 
 
(3) Observations might not be independent of each other

Additionally, observations are treated as independent of each other. However, it is possible which features are predicitve of the unemployment rate have shifted with time. For instance, while the number of technology works might not have a major impact on the unemployment rate of months in 1960, it likely has a larger impact on the unemployment rate of 2020. Thus, it is possible that the variables that are most predictive of unemployment rate across the entire time period are no longer as predictive of the unemployment rate, meaning that the interpretation of our analysis may need to be taken with a grain of salt. 

(4) Dropping features with high multicolinearity 

Since many variables had high multicolinearity, I systematically dropped variables with higher than 0.9 correlation with each other. While I kept 1 of the variables, it is possible that the variable I dropped was most predictive of unemployment rate. However, since the variables dropped are highly correlated with variables left in the analysis, I do not beleive this is a major issue. 


### Analysis limitations

While splitting the data into training and testing datasets allows for a more unbiased test of the models, it also adds randomness to my analysis since observations are randomly assigned to either the test or train data set. It is possible that if I split the data again, without using the same same seed, the variables selected by each model could have changed. 

Next, although I trained my models of 158 variables, my analysis incorporates only variables found in two reports plus inflation and the federal funds rate. The model predicts high unemployment rates in 2020 due to economic factors such net government lending and the number of manufacturing employees. However, high unemployment in 2020 is likely due to health variables not found in my data set. 


## Follow-ups {-}

To compensate for the limitations mentioned above, I would like to use a more up-to-date timeframe. While it is not possible to increase the number of monthly observations for U.S. unemployment rates, I could reproduce my analysis at a county level. To conduct this analyis, I would have to use variables from country-level data sets, instead of data from the Employment Situation or Gross Domestic Product reports. Additionally, I would examine which variables were deleted due to missing values. If possible, I would impute these variables for the missing months so that I do not have to delete so many variables. I would also look into my assumption that monthly unemployment rates are independent of each other. To do so, I would examine patterns in the explantory variables across time and flag any features that appear to have dramatically changed with time. Then, I would either remove these variables or index them. 

\appendix

# Appendix: Descriptions of features {#appendix}

```{r vars-appendix, echo = FALSE}
var_names_1 = var_names[1:37, ]
var_names_2 = var_names[38:70, ]
var_names_3 = var_names[71:100, ]
var_names_4 = var_names[101:135, ]
var_names_5 = var_names[136:159, ]

var_names_1  %>%  
  kable(format = "latex", row.names = NA, 
        booktabs = TRUE) %>%
  kable_styling(position = "left", font_size = 7) %>%
  column_spec(2, width = "9cm") %>%
  column_spec(4, width = "3cm")

var_names_2 %>% 
  kable(format = "latex", row.names = NA, 
        booktabs = TRUE) %>%
  kable_styling(position = "left", font_size = 7) %>%
  column_spec(2, width = "9cm") %>%
  column_spec(4, width = "3cm")

var_names_3 %>%  
  kable(format = "latex", row.names = NA, 
        booktabs = TRUE) %>%
  kable_styling(position = "left", font_size = 7) %>%
  column_spec(2, width = "9cm") %>%
  column_spec(4, width = "3cm")

var_names_4  %>%  
  kable(format = "latex", row.names = NA, 
        booktabs = TRUE) %>%
  kable_styling(position = "left", font_size = 7) %>%
  column_spec(2, width = "9cm") %>%
  column_spec(4, width = "3cm")

var_names_5  %>%  
  kable(format = "latex", row.names = NA, 
        booktabs = TRUE) %>%
  kable_styling(position = "left", font_size = 7) %>%
  column_spec(2, width = "9cm") %>%
  column_spec(4, width = "3cm")
```
