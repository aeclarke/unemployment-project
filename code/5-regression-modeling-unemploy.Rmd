---
title: "5-regression-modeling-unemploy"
author: "Ashley Clarke"
date: "12/14/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Load libraries
```{r, message = FALSE}
library(kableExtra)   # for printing tables
library(cowplot)      # for side by side plots
library(glmnetUtils)  # for running ridge and lasso
library(ISLR2)        # necessary for College data 
library(pROC)         # for ROC curves
library(tidyverse)  
library(dplyr)
library(randomForest)        
```

# Read in the test and training data

```{r}
# load the data
unemploy_train = read_tsv(file = "~/Desktop/STAT471/unemployment-project/data/clean/econ_train.tsv")
unemploy_test = read_tsv(file = "~/Desktop/STAT471/unemployment-project/data/clean/econ_test.tsv")
variable_names = read_tsv(file = "~/Desktop/STAT471/unemployment-project/data/clean/var_names.tsv")

# install.packages("scales")              # dependency of plot_glmnet
source("~/Desktop/STAT471/unemployment-project/code/functions/plot_glmnet.R")
```

Now we will build some predictive models for `unemployment_rate`. I will remove the `date`, `black_unemployment`, and `percent_dif_unemploy` variables from the training and test sets since it is not a feature we will be using for prediction:
```{r}
unemploy_train = unemploy_train %>% select(-c(date, black_unemployment, percent_dif_unemploy))
unemploy_test = unemploy_test %>% select(-c(date, black_unemployment, percent_dif_unemploy))
```

# Run least-squares/ linear regression

```{r lm-fit, echo=FALSE}
#run linear regression
lm_fit = lm(unemployment_rate ~ ., data = unemploy_train)
r_squared_lm = summary(lm_fit)$r.squared
# save the linear regression fit object
save(lm_fit, file = "~/Desktop/STAT471/unemployment-project/results/lm_fit.Rda")
```

# Run ridge regression

```{r ridge-fit, echo=FALSE}
#run ridge fit
set.seed(3) # set seed before cross-validation for reproducibility
ridge_fit = cv.glmnet(unemployment_rate ~ ., alpha = 0, nfolds = 10, data = unemploy_train) 
plot(ridge_fit)

#lambda use 1se rule 
lambda = ridge_fit$lambda.1se
sprintf("The value of lambda based on the one-standard-error rule: %f",
        lambda)

#Visualize the ridge regression fitted coefficients, highlighting 6 features using the `features_to_plot` argument
#Consider: do any features changes signs as lambda increases
plot_glmnet(ridge_fit, unemploy_train, features_to_plot = 6)
save(ridge_fit, file = "~/Desktop/STAT471/unemployment-project/results/ridge_fit.Rda")
```

# Run lasso regression

```{r lasso-fit}
#run lasso fit
set.seed(5) # set seed before cross-validation for reproducibility
lasso_fit = cv.glmnet(unemployment_rate ~ ., alpha = 1, nfolds = 10, data = unemploy_train) 
plot(lasso_fit)

lambda_lasso = lasso_fit$lambda.1se
sprintf("The value of lambda based on the one-standard-error rule: %f",
        lambda_lasso)

#How many features are selected if features are chosen according to the 1 se rule?
num_features = lasso_fit$nzero[lasso_fit$lambda == lasso_fit$lambda.1se]

#Need to add more colors 
#Use `plot_glmnet` to visualize the lasso fitted coefficients, which by default will highlight the features selected by the lasso
plot_glmnet(lasso_fit, unemploy_train, features_to_plot = 8)
```

# Collect Coefficents into a Tibble 
```{r coefs}
#need to add lasso coefs 
lm_coefs_top10 = rownames(data.frame(coef(lm_fit)[-1]) %>% 
  rename(coef = coef.lm_fit...1.) %>% arrange(desc(abs(coef))) %>% head(10))
ridge_coefs_top10 = rownames(data.frame(coef(ridge_fit, s = "lambda.1se")[-1,1]) %>% 
  rename(coef = coef.ridge_fit..s....lambda.1se....1..1.) %>% 
  arrange(desc(abs(coef))) %>% head(10))
lasso_coefs_top10 = rownames(data.frame(coef(lasso_fit, s = "lambda.1se")[-1,1]) %>% 
  rename(coef = coef.lasso_fit..s....lambda.1se....1..1.) %>% 
  arrange(desc(abs(coef))) %>% head(10))

top_10_by_model = tibble("least_squares" = lm_coefs_top10, 
                "ridge" = ridge_coefs_top10, 
                "lasso" = lasso_coefs_top10) 
```

