---
title: "5-regression-modeling-unemploy"
author: "Ashley Clarke"
date: "12/14/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Load libraries
```{r, message = FALSE}
library(kableExtra)   # for printing tables
library(cowplot)      # for side by side plots
library(glmnetUtils)  # for running ridge and lasso
library(ISLR2)        # necessary for College data 
library(pROC)         # for ROC curves
library(tidyverse)  
```

# Read in the test and training data

```{r}
# load the data
setwd("~/Desktop/STAT471/unemployment-project")
unemploy_train = tibble(read_tsv(file = "data/clean/econ_train.tsv"))
unemploy_test = tibble(read_tsv(file = "data/clean/econ_test.tsv"))
variable_names = tibble(read_tsv(file = "data/clean/var_names.tsv"))

# install.packages("scales")              # dependency of plot_glmnet
source("code/functions/plot_glmnet.R")
```

Now we will build some predictive models for `percent_dif_unemploy`. I will remove the `date`, `black_unemployment`, and `unemployment_rate` variables from the training and test sets since it is not a feature we will be using for prediction:
```{r}
unemploy_train = unemploy_train %>% select(-c(date, black_unemployment, percent_dif_unemploy))
unemploy_test = unemploy_test %>% select(-c(date, black_unemployment, percent_dif_unemploy))
```

# Run linear regression

```{r lm-fit, echo=FALSE}
#run linear regression
lm_fit = lm(unemployment_rate ~ ., data = unemploy_train)
r_squared_lm = summary(lm_fit)$r.squared
# save the linear regression fit object
save(lm_fit, file = "~/Desktop/STAT471/unemployment-project/results/ridge_fit.Rda")
```

# Run ridge regression

```{r ridge-fit, echo=FALSE}
#run ridge fit
set.seed(3) # set seed before cross-validation for reproducibility
ridge_fit = cv.glmnet(unemployment_rate ~ ., alpha = 0, nfolds = 10, data = unemploy_train) 
plot(ridge_fit)

#lambda use 1se rule 
lambda = ridge_fit$lambda.1se
sprintf("The value of lambda based on the one-standard-error rule: %f",
        lambda)

#Visualize the ridge regression fitted coefficients, highlighting 6 features using the `features_to_plot` argument
#Consider: do any features changes signs as lambda increases
plot_glmnet(ridge_fit, unemploy_train, features_to_plot = 6)
```

# Run lasso regression

```{r lasso-fit}
#run lasso fit
set.seed(5) # set seed before cross-validation for reproducibility
lasso_fit = cv.glmnet(unemployment_rate ~ ., alpha = 1, nfolds = 10, data = unemploy_train) 
plot(lasso_fit)

lambda_lasso = lasso_fit$lambda.1se
sprintf("The value of lambda based on the one-standard-error rule: %f",
        lambda_lasso)

#How many features are selected if features are chosen according to the 1 se rule?
num_features = lasso_fit$nzero[lasso_fit$lambda == lasso_fit$lambda.1se]

#Need to add more colors 
#Use `plot_glmnet` to visualize the lasso fitted coefficients, which by default will highlight the features selected by the lasso
plot_glmnet(lasso_fit, unemploy_train)
```

# Errors for model
```{r error-for-models-table}
# generate predictions if guess mean difference
mean_dif = unemploy_train %>% summarise(mean(unemployment_rate)) %>% pull()
mean_predictions = rep(mean_dif, nrow(unemploy_test))
mean_RMSE = sqrt(mean((mean_predictions - unemploy_test$unemployment_rate)^2))

# generate linear model predictions and test error
linear_predictions = predict(lm_fit, newdata = unemploy_test)
linear_RMSE = sqrt(mean((linear_predictions - unemploy_test$unemployment_rate)^2))

# generate ridge regression model predictions and test error
ridge_predictions = predict(ridge_fit, 
                            newdata = unemploy_test, 
                            s = "lambda.1se")
ridge_RMSE = sqrt(mean((ridge_predictions - unemploy_test$unemployment_rate)^2))

# generate lasso regression model predictions and test error
lasso_predictions = predict(lasso_fit, 
                            newdata = unemploy_test,
                            s = "lambda.1se")
lasso_RMSE = sqrt(mean((lasso_predictions -  unemploy_test$unemployment_rate)^2))

# create table of these three model test errors
error_for_models = tribble(
  ~Model, ~RMSE, 
  #------/------- 
  "Linear", linear_RMSE,
  "Ridge", ridge_RMSE,
  "Lasso", lasso_RMSE,
  )

# print these metrics in nice table
error_for_models %>% kable(format = "latex", row.names = NA, 
                        booktabs = TRUE,
                        digits = 5,
                        col.names = c("Model type", 
                                      "Root mean squared error")) %>%
  kable_styling(position = "center") %>%
  kable_styling(latex_options = "HOLD_position")  %>%
  save_kable(file =
               "~/Desktop/STAT471/unemployment-project/results/RMSE-models.pdf", 
             self_contained = T)
```

# Collect Coefficents into a Tibble 
```{r coefs, echo=FALSE}
#need to add lasso coefs 
coeffs = tibble(lm_coef = coef(lm_fit)[-1],
                ridge_coef = coef(ridge_fit, s = "lambda.1se")[-1,1],
                features = names(coef(lm_fit)[-1])) 
coeffs
```
## Training a random forest

To train a random forest with default settings, we use the following syntax:
```{r}
rf_fit = randomForest(unemployment_rate ~ ., data = unemploy_train)
# OOB error as function of number of trees 
plot(rf_fit)

rf_fit = randomForest(unemployment_rate ~ ., mtry = 19, data = Hitters_train)
plot(rf_fit)
```

We can get a quick visualization by using `plot`, which shows us the OOB error as a function of the number of trees. 
```{r}
plot(rf_fit)
```